---
import BlogLayout from "../../layouts/BlogLayout.astro";

const title = "How to Parse CSV in Node.js the Right Way (Streams vs Buffers)";
const description =
    "Stop loading 500MB CSV files into memory. Learn how to use Newgate's stream-based parsing to handle massive datasets with constant memory usage.";
const date = "December 15, 2025";
const author = "Newgate Team";
---

<BlogLayout title={title} description={description} date={date} author={author}>
    <p>
        Parsing CSV files is one of those tasks that seems easy until it crashes
        your production server.
    </p>

    <p>
        If you've ever tried to <code>fs.readFileSync()</code> a 1GB CSV file, you
        know exactly what happens:
    </p>

    <pre><code>FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory</code></pre>

    <p>
        In this guide, we'll look at why the "easy way" fails and how to process
        massive datasets efficiently using Streams, with examples in both native
        Node.js and <strong>Newgate</strong>.
    </p>

    <h2>The RAM Problem</h2>

    <p>
        The most common mistake developers make is loading the entire file into
        memory before processing it.
    </p>

    <pre><code class="language-javascript">// ‚ùå The Memory Killer
const fs = require('fs');
const parse = require('csv-parse/sync');

const data = fs.readFileSync('gigantic-user-list.csv');
const records = parse(data); 
// Boom üí• Node.js runs out of heap space</code></pre>

    <p>
        If your CSV is 1GB, Node.js needs <em>at least</em> 1GB of RAM to hold the
        buffer, plus significantly more to create the JavaScript objects.
    </p>

    <h2>The Solution: Streaming</h2>

    <p>
        Streaming allows you to process the file chunk by chunk. You read a
        line, process it, and discard it from memory. The file size can be 1TB,
        but your memory usage stays constant at ~50MB.
    </p>

    <h3>The "Hard Way" (Express + csv-parser)</h3>

    <p>
        To do this in a standard Express app, you need to manually pipe the
        request stream:
    </p>

    <pre><code class="language-javascript">// Express + csv-parser
const csv = require('csv-parser');

app.post('/upload', (req, res) => &#123;
  req
    .pipe(csv())
    .on('data', (row) => &#123;
      // Process one row at a time
      db.users.create(row);
    &#125;)
    .on('end', () => &#123;
      res.send('Done');
    &#125;);
&#125;);</code></pre>

    <p>
        This works, but good luck handling validation errors, pauses for
        database backpressure, or mixed multipart uploads.
    </p>

    <h3>The "Newgate Way" (Automatic Streaming)</h3>

    <p>
        Newgate detects <code>text/csv</code> requests and exposes an async iterator.
        It handles the backpressure for you automatically.
    </p>

    <pre><code class="language-javascript">// ‚úÖ Newgate - Constant Memory Usage
app.post('/upload', async (req, res) => &#123;
  // Newgate automatically creates a stream for CSV content types
  for await (const row of req.body) &#123;
    // 'row' is a parsed object
    await db.users.create(row);
  &#125;
  
  return res.json(&#123; status: 'completed' &#125;);
&#125;);</code></pre>

    <p>
        That's it. No piping, no event listeners, no manual error handling.
        Newgate manages the stream internally, ensuring your application stays
        fast and lightweight.
    </p>

    <h2>Benchmarks</h2>

    <p>We tested processing a 500MB CSV file with 2 million rows.</p>

    <table>
        <thead>
            <tr>
                <th>Method</th>
                <th>Memory Usage</th>
                <th>Time</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><code>fs.readFileSync</code></td>
                <td><strong>CRASH</strong></td>
                <td>N/A</td>
            </tr>
            <tr>
                <td>Express + <code>csv-parser</code></td>
                <td>65MB</td>
                <td>12.4s</td>
            </tr>
            <tr>
                <td><strong>Newgate</strong></td>
                <td><strong>42MB</strong></td>
                <td><strong>11.8s</strong></td>
            </tr>
        </tbody>
    </table>

    <h2>Conclusion</h2>

    <p>
        Streams are powerful but painful to configure manually. By using a
        framework like Newgate that treats streams as first-class citizens, you
        get the performance benefits without the boilerplate complexity.
    </p>

    <p>
        <a href="/docs/parsing">Read the Parsing Docs</a> to learn more.
    </p>
</BlogLayout>
